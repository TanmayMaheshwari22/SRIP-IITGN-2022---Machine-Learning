{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b371860",
   "metadata": {},
   "source": [
    "Question-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb9d95",
   "metadata": {},
   "source": [
    "jax.numpy-Implements the NumPy API, using the primitives in jax.lax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49dc232",
   "metadata": {},
   "source": [
    "jax.scipy.special.logsumexp(a, axis=None, b=None, keepdims=False, return_sign=False)[source]\n",
    "Compute the log of the sum of exponentials of input elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807120cd",
   "metadata": {},
   "source": [
    "JAX runs transparently on the GPU or TPU (falling back to CPU if you don’t have one). \n",
    "However, in the above example, JAX is dispatching kernels to the GPU one operation at a time.\n",
    "If we have a sequence of operations, we can use the @jit decorator to compile multiple operations together using XLA. Let’s try that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e92390",
   "metadata": {},
   "source": [
    "Taking derivatives with grad()\n",
    "In addition to evaluating numerical functions, we also want to transform them. \n",
    "One transformation is automatic differentiation. \n",
    "In JAX, just like in Autograd, you can compute gradients with the grad() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dff1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_logistic(x):\n",
    "      return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.25 0.19661197 0.10499357]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811b34f",
   "metadata": {},
   "source": [
    "Auto-vectorization with vmap()\n",
    "JAX has one more transformation in its API that you might find useful: vmap(), the vectorizing map. \n",
    "It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, \n",
    "it pushes the loop down into a function’s primitive operations for better performance. \n",
    "When composed with jit(), it can be just as fast as adding the batch dimensions by hand.\n",
    "\n",
    "We’re going to work with a simple example, and promote matrix-vector products into matrix-matrix products using vmap(). \n",
    "Although this is easy to do by hand in this specific case, the same technique can apply to more complicated functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fcb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "      return jnp.dot(mat, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17856e26",
   "metadata": {},
   "source": [
    "Of course, vmap() can be arbitrarily composed with jit(), grad(), and any other JAX transformation.\n",
    "This is just a taste of what JAX can do. We’re really excited to see what you do with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0de87",
   "metadata": {},
   "source": [
    "All datasets are subclasses of torch.utils.data.Dataset i.e, they have __getitem__ and __len__ methods implemented. Hence, they can all be passed to a torch.utils.data.DataLoader which can load multiple samples in parallel using torch.multiprocessing workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bed7ce",
   "metadata": {},
   "source": [
    "All the datasets have almost similar API. They all have two common arguments: transform and target_transform to transform the input and target respectively. You can also create your own datasets using the provided base classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190a6f9",
   "metadata": {},
   "source": [
    "DATASETS & DATALOADERS\n",
    "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
    "\n",
    "PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass torch.utils.data.Dataset and implement functions specific to the particular data. They can be used to prototype and benchmark your model. You can find them here: Image Datasets, Text Datasets, and Audio Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6f5e73",
   "metadata": {},
   "source": [
    "Loading a Dataset\n",
    "Here is an example of how to load the Fashion-MNIST dataset from TorchVision. Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.\n",
    "\n",
    "We load the FashionMNIST Dataset with the following parameters:\n",
    "root is the path where the train/test data is stored,\n",
    "train specifies training or test dataset,\n",
    "download=True downloads the data from the internet if it’s not available at root.\n",
    "transform and target_transform specify the feature and label transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a72e5",
   "metadata": {},
   "source": [
    "jax.random.PRNGKey\n",
    "jax.random.PRNGKey(seed)[source]\n",
    "Create a pseudo-random number generator (PRNG) key given an integer seed\n",
    "Parameters\n",
    "seed (int) – a 64- or 32-bit integer used as the value of the key.\n",
    "Return type-Union[Any, PRNGKeyArray]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ea679",
   "metadata": {},
   "source": [
    "Often, we want to operate on objects that look like dicts of arrays, or lists of lists of dicts, or other nested structures. In JAX, we refer to these as pytrees, but you can sometimes see them called nests, or just trees.\n",
    "\n",
    "JAX has built-in support for such objects, both in its library functions as well as through the use of functions from jax.tree_utils (with the most common ones also available as jax.tree_*). This section will explain how to use them, give some useful snippets and point out common gotchas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a408929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
